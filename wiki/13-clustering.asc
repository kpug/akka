== 13. Clustering

:numbered!:

In this chapter:

* Cluster Membership
* Cluster Aware Routers
* Cluster Patterns

=== 13.1 Why use clustering?

아래와 같이 생긴 클러스터링 시스템을 만들어 본다. 진짜 한땀한땀 사람이 만들어 줘야 하는 거다.

image::img/figure-13-1.png[]

* Cluster의 ActorSystem은 전부 **같은 이름**이다.
* 서로의 상태에 대해 Gossip한다.

Cluster 모듈이 현재 지원하는 것:

* Cluster membership—Fault tolerant membership for actor systems.
* Load balancing—Routing messages to actors in the cluster based on a routing algorithm.
* Node partitioning—A node can be given a specific role in the cluster. Routers can be configured to only send messages to nodes with a specific role.
* Partition points—An actor system can be partitioned in actor sub-trees that are located on different nodes. Right now only top-level partition points are supported. This means that you can only access top level actors on nodes in the cluster using routers.

안되는 건(failover, re-balancing, re-partitioning, replication of state) 안된다.

이 장에서 내내 설명하는 구성:

image::img/figure-13-2.png[]

. Job Receptionist가 job Request를 받으면
.. Job Master를 만든다.
.. Job Master에게 `StartJob`을 보내서 일을 시킨다.
... Job Master는 Worker를 만든다.
... Job Master는 `Work`를 1초마다 Worker에 보낸다.
.... Worker는 `Enlist`와 `NextTask`를 JobMaster에 보낸다.
.... Master는 남은 일이 있으면 `Task`를 보낸다.
..... Worker는 일을 하고 TaskResult를 Master에 보낸다.
.... Master는 남은 일이 없으면 `WorkLoadDepleted`를 보낸다.
..... Worker는 스스로 stop한다.
... Job Master는 결과를 취합해서 `MergeResult`를 Receptionist에 보낸다.


=== 13.2 Cluster membership

Cluster Note는 Role로 구분한다.

* Seed: 클러스터 관리. Seed중 한 Node가 Leader가 된다.
* Master: Job 요청을 처리하고 관리
* Worker: 실제 Job 수행.

이 장에서 살펴보는 노드 구성:

image::img/figure-13-3.png[]

==== 13.2.1 Joining the cluster

첫 번째 Seed가 Leader가 됨, 첫 번째 Seed가 Leader가 되는 단순한 알고리즘. 그래서 각 Seed가 각자 Cluster를 만들지 않음..

image::img/figure-13-4.png[]

* Cluster는 zero-config discovery protocol을 지원하지 않음(Seed node의 목록을 전부 설정해 줘야 함)
* 첫 Seed은 Cluster를 만들고 Join한다. 그리고 Leader가 됨.

[NOTE]
====
Cluster에 Join하는 방법(그러니까 Cluster를 관리하는 건 개발자가 알아서 응!?)

* Well-known ip를 사용하는 seed node를 사용해서 직접 만든다.
* Cluster discovery protocol을 이용해서 직접 만든다.
* 이미 있는 discovery/register 기술을 이용한다. Apache Zookeeper, Hashicorp consul, coreos/etcd, glue 같은 걸 이용한다.
====

image::img/figure-13-5.png[]

`akka-cluster` 모듈을 추가한다:

[source, scala]
----
com.typesafe.akka %% akka-cluster & akkaVersion
----

[source, scala]
----
akka {
    loglevel = INFO
    stdout-loglevel = INFO
    event-handlers = ["akka.event.Logging$DefaultLogger"]
    log-dead-letters = 0
    log-dead-letters-during-shutdown = off
    actor {
        provider = "akka.cluster.ClusterActorRefProvider" // <1>
    }
    remote { // <2>
        enabled-transports = ["akka.remote.netty.tcp"]
        log-remote-lifecycle-events = off // <3>
        netty.tcp {
            hostname = "127.0.0.1"
            hostname = ${?HOST} <4>
            port = ${PORT}
        }
    }
    cluster { // <5>
        seed-nodes = [
            "akka.tcp://words@127.0.0.1:2551",
            "akka.tcp://words@127.0.0.1:2552",
            "akka.tcp://words@127.0.0.1:2553"
        ] // <6>
        roles = ["seed"] // <7>
        role {
            seed.min-nr-of-members = 1 // <8>
        }
    }
}
----
<1> Remote Deploy는 `akka.remote.RemoteActorRefProvider` 였었는데, Cluster는 `akka.cluster.ClusterActorRefProvider`.
<2> Seed Node의 Remote 설정
<3> `log-remote-lifecycle-events` 리모트의 Actor 로그가 꺼진다는 의미 겠지?
<4> HOST 변수가 정의돼 있으면 그 변수를 사용하고 없으면 스킵.
<5> cluster 설정
<6> seed 노드 선언
<7> 이 설정 파일을 사용하는 Node의 Role
<8> 최소 work node의 수. 이 수 만큼 Worker node가 Up되면 registerOnMemberUp이 호출 되기 때문에, Worker가 준비 됀 상태를 판단 할 수 있다.

image::img/figure-13-6.png[]

* 첫번째 Seed 노드는 Cluster를 만들고 Join한다.

image::img/figure-13-7.png[]

* 두번째/세번째 Seed 노드는 Join 메시즈를 보내고 Joining State가 된다.
* Leader가 노드를 Joining State에서 Up State로 만든다.

==== 13.2.2 Leaving the cluster

image::img/figure-13-8.png[]

* Seed 1은 Leader니까 스스로 Leaving, Exiting 상태로 변환하고 다른 모든 노드와 상태공유.
* Seed 1이 Shutdown하면 다른 Seed node들이 Unreachable임을 감지.
* 설정에 따라 두번째 Seed가 Leader가 되고 Seed 1을 Removed State로 변경한다.

[NOTE]
.Gossip Protocol
====
Node끼리 내부적으로 떠드는데 이걸 Gosip Protocol이라고 부른다.

* 모든 Node는 자신의 State와 다른 Node의 State를 주고 받는다.
* 그래서 모든 Node는 모든 Node의 State를 Eventually agree(안다)한다.
* 모든 노드가 Eventully Agreement하게 되면 이걸 Convergence라고 부른다.
* Cluster의 Leader는 Convergence 후에 선출(설정한 순서 대로)된다.
====

image::img/figure-13-9.png[]

* 한번 Leave한 Seed 노드는 다시 Join할 수 없다. 다시 Join하려면 ActorSystem을 다시 만들어야 한다.

image::img/figure-13-10.png[]

* Leader가 Unreachable임을 detect하면 어떤 Node에도 Leader Action을 실행할 수 없다.
* `akka.cluster.auto-down-unreachable-after` 설정 후에 Leader 새로 선출되면 Unreachable Node는 Down되고 제거된다.
* (이렇게 말하는 것 같은데 책에 정확한 워딩이 없다)

[NOTE]
.Failure Detector
====
Unreachable Node를 Detect하는데 Accrual Phi Failure Detector라는 걸 사용한다.

이 Detector는 Unreachable로 추측될 때 마다 phi value를 계산하는데 임계치에(suspicion level) 다다르면 Unreachable로 판단한다. `akka.cluster.failure-detector` 에서 설정할 수 있다.
====

Cluster에서 fail하는 Node가 생기면 Notification을 받고 싶을 테다. Cluster Extension에 subscribe할 수 있다.

.ClusterDomainEventListner
[source, scala]
----
import akka.cluster.{MemberStatus, Cluster}
import akka.cluster.ClusterEvent._

class ClusterDomainEventListener extends Actor with ActorLogging {
    Clustre(context.system).subscribe(self, classOf[ClusterDomainEvent]) // <1>

    def receive ={ // <2>
        case MemberUp(member) => log.info(s"$member UP.")
        case MemberExited(member)=> log.info(s"$member EXITED.")
        case MemberRemoved(m, previousState) =>
            if(previousState == MemberStatus.Exiting) {
                log.info(s"Member $m gracefully exited, REMOVED.")
            } else {
                log.info(s"$m downed after unreachable, REMOVED.")
            }
        case UnreachableMember(m) => log.info(s"$m UNREACHABLE")
        case ReachableMember(m) => log.info(s"$m REACHABLE")
        case s: CurrentClusterState => log.info(s"cluster state: $s")
    }

    override def postStop(): Unit = {
        Cluster(context.system).unsubscribe(self) // <3>
        super.postStop()
    }
}
----
<1> subscribe!
<2> ClusterDomainEvent 들...
<3> Actor가 stop하면 Unsubscribe한다.

그외 다른 이벤트도 있는데 책에서 소개 하지 않는다.

=== 13.3 Clustered job processing

이 그림을 다시 첨부하지만 자세한 설명은 생략:

image::img/figure-13-11.png[]

이 순서대로 메시지를 주고 받는다:

image::img/figure-13-12.png[]

==== 13.3.1 Starting the cluster

.같은 패키지로 설정만 다르게 해서 Node를 실행시킨다.
[source, scala]
----
java -DPORT=2551 \
    -Dconfig.resource=/seed.conf \
    -jar target/words-node.jar
java -DPORT=2554 \
    -Dconfig.resource=/master.conf \
    -jar target/words-node.jar
java -DPORT=2555 \
    -Dconfig.resource=/worker.conf \
    -jar target/words-node.jar
java -DPORT=2556 \
    -Dconfig.resource=/worker.conf \
    -jar target/words-node.jar
----

[NOTE]
====
Worker의 결과를 메모리에 저장하지만 Hadoop처럼 중간결과를 어딘가에 저장(Persistent하게)하는 게 좋다.
그런건 직접해.~~
====

[NOTE]
====
`-Dakka.cluster.seed-nodes.[n]=[seednode]` 옵션을 주고 실행할 때 변경할 수도 있다.
n은 zero-based index.
====

.registerOnMemberUp
[source, scala]
----
role {
    worker.min-nr-of-members = 2 // <1>
}

object Main extends App {
    val config = ConfigFactory.load()
    val system = ActorSystem("words", config)

    println(s"Starting node with roles: ${Cluster(system).selfRoles}")

    val roles = system.settings
        .config
        .getStringList("akka.cluster.roles")

    if(roles.contains("master")) { // <2>
        Cluster(system).registerOnMemberUp { //<3>
            val receptionist = system.actorOf(Props[JobReceptionist],
                    "receptionist") // <4>
            println("Master node is ready.")
        }
    }
}
----
<1> Worker Node가 최소 2개가 되어야 registerOnMemberUp이 호출된다.
<2> "master" Node만 적용
<3> registerOnMemberUp callback.
<4> receptionlist를 만들어서 JobRequest를 받는다. 이미 Worker Node가 2개 올라온 상태이므로 JobRequest를 처리할 수 있다.

==== 13.3.2 Work distribution using routers

.Code로 만드는 Cluster Router, 그런데 설정으로 할 수 있다고...
[source, scala]
----
trait CreateWorkerRouter { this: Actor => // <1>
    def createWorkerRouter: ActorRef = {
        context.actorOf(
            ClusterRouterPool(BroadcastPool(10), // <2>
                ClusterRouterPoolSettings(
                    totalInstances = 1000,  // <3>
                    maxInstancesPerNode = 20, // <4>
                    allowLocalRoutees = false, // <5>
                    useRole = None // <6>
                    )
                ).props(Props[JobWorker]), // <7>
        name = "worker-router")
    }
}
----
<1> Actor에 Mixin할 거임.
<2> Routee에 Broadcast하는 ClusterRouter를 만든다.
<3> worker는 1000개까지 만든다.
<4> Node당 Worker는 최대 20개.
<5> Local Node에는 Routee를 만들지 않는다.
<6> 'Nodes with this role will be routed to'라고 하는데 이게 뭔말임!
<7> JobWorker를 Routee로 만든다.

직접 Worker를 관리하고 Deploy하는 코드를 만들지 않아도 된다.

[source,scala]
----
class JobMaster extends Actor
        with ActorLogging
        with CreateWorkerRouter { // <1>

    // inside the body of the JobMaster actor..
    val router = createWorkerRouter // <2>
    def receive = idle

    def idle: Receive = {
        case StartJob(jobName, text) =>
            textParts = text.grouped(10).toVector
            val cancel = system.scheduler.schedule(0 millis,
                    1000 millis, // <3>
                    router,
                    Work(jobName, self))

            become(working(jobName, sender(), cancel))
    }

    // more code
----
<1> CreateWorkerRouter를 mixin
<2> router 생성
<3> 1초마다 Routee에 Work 메시지를 보낸다.

* Work 메시지를 받은 Worker가 Master에 일을 달라고 요청한다.
* Worker가 Task를 하나 끝내고 Master에 NextTask를 요청하지 않는다.
* Work가 NextTask를 요청하도록 Master가 Work 메시지를 주기적으로 보내는 구조이기 때문에 자동으로 Balanced된다.

==== 13.3.3 Resillient Jobs

* Enlist메시지에 Worker의 ActorRef를 실어서 보낸다. 그래서 Master는 모든 Worker를 Watch할 수 있다.
** 그래서 일이 다 끝나면 Master는 모든 Worker를 죽일 수 있다.
* 반대로 Worker도 Master를 Watch하고 있다가 Master가 죽으면 자살한다.

이하 긴 소스를 보여주고 여지것 설명한 것을 구현하는 것을 보여주는데, 다음 스터디 때 이어서 해야 겠다. 아 길다..

==== 13.3.4 Testing the cluster

=== 13.4 Summary
